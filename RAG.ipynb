{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use repo provided from Anthropic: {https://github.com/anthropics/anthropic-cookbook/tree/main/skills/contextual-embeddings}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Obtaining dependency information for faiss-cpu from https://files.pythonhosted.org/packages/2c/2d/d2a4171a9cca9a7c04cd9d6f9441a37f1e0558724b90bf7fc7db08553601/faiss_cpu-1.10.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading faiss_cpu-1.10.0-cp311-cp311-win_amd64.whl.metadata (4.5 kB)\n",
      "Collecting sentence-transformers\n",
      "  Obtaining dependency information for sentence-transformers from https://files.pythonhosted.org/packages/05/89/7eb147a37b7f31d3c815543df539d8b8d0425e93296c875cc87719d65232/sentence_transformers-3.4.1-py3-none-any.whl.metadata\n",
      "  Downloading sentence_transformers-3.4.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers in c:\\users\\bdion\\anaconda3\\lib\\site-packages (4.38.0)\n",
      "Requirement already satisfied: openai in c:\\users\\bdion\\anaconda3\\lib\\site-packages (0.28.0)\n",
      "Collecting numpy<3.0,>=1.25.0 (from faiss-cpu)\n",
      "  Obtaining dependency information for numpy<3.0,>=1.25.0 from https://files.pythonhosted.org/packages/b9/c6/cd4298729826af9979c5f9ab02fcaa344b82621e7c49322cd2d210483d3f/numpy-2.2.3-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading numpy-2.2.3-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 60.8/60.8 kB 3.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from faiss-cpu) (23.1)\n",
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/20/37/1f29af63e9c30156a3ed6ebc2754077016577c094f31de7b2631e5d379eb/transformers-4.49.0-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 44.0/44.0 kB 2.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.11.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.29.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from sentence-transformers) (9.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.22,>=0.21 from https://files.pythonhosted.org/packages/44/69/d21eb253fa91622da25585d362a874fa4710be600f0ea9446d8d0217cec1/tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from openai) (3.8.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: networkx in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from aiohttp->openai) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from aiohttp->openai) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.2.0)\n",
      "Collecting numpy<3.0,>=1.25.0 (from faiss-cpu)\n",
      "  Obtaining dependency information for numpy<3.0,>=1.25.0 from https://files.pythonhosted.org/packages/3f/6b/5610004206cf7f8e7ad91c5a85a8c71b2f2f8051a0c0c4d5916b76d6cbb2/numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata (61 kB)\n",
      "     ---------------------------------------- 0.0/61.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 61.0/61.0 kB 3.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.1)\n",
      "Downloading faiss_cpu-1.10.0-cp311-cp311-win_amd64.whl (13.7 MB)\n",
      "   ---------------------------------------- 0.0/13.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.4/13.7 MB 5.9 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.9/13.7 MB 6.8 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 1.2/13.7 MB 6.8 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.5/13.7 MB 6.7 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.9/13.7 MB 6.9 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.4/13.7 MB 7.7 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.9/13.7 MB 7.7 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 3.5/13.7 MB 8.4 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 4.4/13.7 MB 8.7 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 5.2/13.7 MB 9.5 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 5.9/13.7 MB 9.9 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 6.8/13.7 MB 10.3 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 7.5/13.7 MB 10.6 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 7.9/13.7 MB 10.6 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 8.4/13.7 MB 10.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 9.2/13.7 MB 10.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 9.9/13.7 MB 10.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 10.8/13.7 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 11.7/13.7 MB 12.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 12.8/13.7 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.5/13.7 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.7/13.7 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.7/13.7 MB 11.3 MB/s eta 0:00:00\n",
      "Downloading sentence_transformers-3.4.1-py3-none-any.whl (275 kB)\n",
      "   ---------------------------------------- 0.0/275.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 275.9/275.9 kB 8.6 MB/s eta 0:00:00\n",
      "Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "   ---------------------------------------- 0.0/10.0 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.9/10.0 MB 14.5 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 1.8/10.0 MB 16.4 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 2.8/10.0 MB 14.8 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 4.5/10.0 MB 11.6 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.3/10.0 MB 11.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.8/10.0 MB 13.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.8/10.0 MB 13.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.2/10.0 MB 15.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.0/10.0 MB 15.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.0/10.0 MB 15.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.0/10.0 MB 13.0 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 0.9/2.4 MB 18.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.7/2.4 MB 17.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.4/2.4 MB 17.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 11.7 MB/s eta 0:00:00\n",
      "Downloading numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "   ---------------------------------------- 0.0/15.8 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.7/15.8 MB 23.1 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 1.4/15.8 MB 17.2 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 2.3/15.8 MB 16.2 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 3.2/15.8 MB 15.6 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 4.1/15.8 MB 15.5 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 4.7/15.8 MB 15.8 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 5.5/15.8 MB 15.9 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 6.2/15.8 MB 15.9 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 6.9/15.8 MB 15.3 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 7.8/15.8 MB 15.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 8.7/15.8 MB 15.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 9.3/15.8 MB 15.2 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 9.7/15.8 MB 15.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 10.4/15.8 MB 14.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 11.1/15.8 MB 14.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 11.8/15.8 MB 14.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 12.3/15.8 MB 14.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 13.0/15.8 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 13.5/15.8 MB 13.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 14.0/15.8 MB 13.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 14.7/15.8 MB 13.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.7/15.8 MB 12.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.8/15.8 MB 12.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.8/15.8 MB 12.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.8/15.8 MB 12.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.8/15.8 MB 12.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.8/15.8 MB 10.2 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy, faiss-cpu, tokenizers, transformers, sentence-transformers\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.3\n",
      "    Uninstalling numpy-1.24.3:\n",
      "      Successfully uninstalled numpy-1.24.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\bdion\\\\anaconda3\\\\Lib\\\\site-packages\\\\~umpy\\\\core\\\\_multiarray_tests.cp311-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting anthropic\n",
      "  Obtaining dependency information for anthropic from https://files.pythonhosted.org/packages/92/ad/feddd3ed83804b7f05c90b343e2d9df8f4a28028d6820c1a034de79dcdab/anthropic-0.47.2-py3-none-any.whl.metadata\n",
      "  Downloading anthropic-0.47.2-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from anthropic) (3.5.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from anthropic) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from anthropic) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from anthropic) (0.7.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from anthropic) (1.10.8)\n",
      "Requirement already satisfied: sniffio in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from anthropic) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from anthropic) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->anthropic) (3.4)\n",
      "Requirement already satisfied: certifi in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->anthropic) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->anthropic) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic) (0.14.0)\n",
      "Downloading anthropic-0.47.2-py3-none-any.whl (239 kB)\n",
      "   ---------------------------------------- 0.0/239.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 239.5/239.5 kB 7.4 MB/s eta 0:00:00\n",
      "Installing collected packages: anthropic\n",
      "Successfully installed anthropic-0.47.2\n",
      "Collecting voyageai\n",
      "  Obtaining dependency information for voyageai from https://files.pythonhosted.org/packages/86/e1/0b2defa3a83aabe67db05d5f494d617dd4764b2a043d83ddc26be5e6e0db/voyageai-0.3.2-py3-none-any.whl.metadata\n",
      "  Downloading voyageai-0.3.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from voyageai) (3.8.5)\n",
      "Collecting aiolimiter (from voyageai)\n",
      "  Obtaining dependency information for aiolimiter from https://files.pythonhosted.org/packages/f3/ba/df6e8e1045aebc4778d19b8a3a9bc1808adb1619ba94ca354d9ba17d86c3/aiolimiter-1.2.1-py3-none-any.whl.metadata\n",
      "  Downloading aiolimiter-1.2.1-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from voyageai) (1.26.4)\n",
      "Requirement already satisfied: pillow in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from voyageai) (9.4.0)\n",
      "Requirement already satisfied: pydantic>=1.10.8 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from voyageai) (1.10.8)\n",
      "Requirement already satisfied: requests in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from voyageai) (2.31.0)\n",
      "Requirement already satisfied: tenacity in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from voyageai) (8.2.2)\n",
      "Requirement already satisfied: tokenizers>=0.14.0 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from voyageai) (0.15.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from pydantic>=1.10.8->voyageai) (4.12.2)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from tokenizers>=0.14.0->voyageai) (0.29.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from aiohttp->voyageai) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from aiohttp->voyageai) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from aiohttp->voyageai) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from aiohttp->voyageai) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from aiohttp->voyageai) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from aiohttp->voyageai) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from aiohttp->voyageai) (1.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from requests->voyageai) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from requests->voyageai) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from requests->voyageai) (2025.1.31)\n",
      "Requirement already satisfied: filelock in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.14.0->voyageai) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.14.0->voyageai) (2025.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.14.0->voyageai) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.14.0->voyageai) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.14.0->voyageai) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub<1.0,>=0.16.4->tokenizers>=0.14.0->voyageai) (0.4.6)\n",
      "Downloading voyageai-0.3.2-py3-none-any.whl (25 kB)\n",
      "Downloading aiolimiter-1.2.1-py3-none-any.whl (6.7 kB)\n",
      "Installing collected packages: aiolimiter, voyageai\n",
      "Successfully installed aiolimiter-1.2.1 voyageai-0.3.2\n",
      "Collecting cohere\n",
      "  Obtaining dependency information for cohere from https://files.pythonhosted.org/packages/50/c6/cffec9284d9713d28c6235a653a9a34c49b0f880f00cfa002252cdb8d033/cohere-5.13.12-py3-none-any.whl.metadata\n",
      "  Downloading cohere-5.13.12-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting fastavro<2.0.0,>=1.9.4 (from cohere)\n",
      "  Obtaining dependency information for fastavro<2.0.0,>=1.9.4 from https://files.pythonhosted.org/packages/fd/7f/21711a9ec9937c84406e0773ba3fc6f8d66389a364da46618706f9c37d30/fastavro-1.10.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading fastavro-1.10.0-cp311-cp311-win_amd64.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: httpx>=0.21.2 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from cohere) (0.27.2)\n",
      "Collecting httpx-sse==0.4.0 (from cohere)\n",
      "  Obtaining dependency information for httpx-sse==0.4.0 from https://files.pythonhosted.org/packages/e1/9b/a181f281f65d776426002f330c31849b86b31fc9d848db62e16f03ff739f/httpx_sse-0.4.0-py3-none-any.whl.metadata\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: pydantic>=1.9.2 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from cohere) (1.10.8)\n",
      "Collecting pydantic-core<3.0.0,>=2.18.2 (from cohere)\n",
      "  Obtaining dependency information for pydantic-core<3.0.0,>=2.18.2 from https://files.pythonhosted.org/packages/08/68/5c68a99cab786d14bfd85aa711c8147965a109a7250e97aecc51708322cd/pydantic_core-2.29.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading pydantic_core-2.29.0-cp311-cp311-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from cohere) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<1,>=0.15 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from cohere) (0.15.2)\n",
      "Collecting types-requests<3.0.0,>=2.0.0 (from cohere)\n",
      "  Obtaining dependency information for types-requests<3.0.0,>=2.0.0 from https://files.pythonhosted.org/packages/d7/01/485b3026ff90e5190b5e24f1711522e06c79f4a56c8f4b95848ac072e20f/types_requests-2.32.0.20241016-py3-none-any.whl.metadata\n",
      "  Downloading types_requests-2.32.0.20241016-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.0.0 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from cohere) (4.12.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from httpx>=0.21.2->cohere) (3.5.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from httpx>=0.21.2->cohere) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from httpx>=0.21.2->cohere) (1.0.6)\n",
      "Requirement already satisfied: idna in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from httpx>=0.21.2->cohere) (3.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from httpx>=0.21.2->cohere) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.21.2->cohere) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->cohere) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->cohere) (1.26.16)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from tokenizers<1,>=0.15->cohere) (0.29.1)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0,>=2.0.0->cohere)\n",
      "  Obtaining dependency information for urllib3<3,>=1.21.1 from https://files.pythonhosted.org/packages/c8/19/4ec628951a74043532ca2cf5d97b7b14863931476d117c471e8e2b1eb39f/urllib3-2.3.0-py3-none-any.whl.metadata\n",
      "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (2025.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (0.4.6)\n",
      "Downloading cohere-5.13.12-py3-none-any.whl (252 kB)\n",
      "   ---------------------------------------- 0.0/252.9 kB ? eta -:--:--\n",
      "   --------------------------- ------------ 174.1/252.9 kB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 252.9/252.9 kB 3.9 MB/s eta 0:00:00\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading fastavro-1.10.0-cp311-cp311-win_amd64.whl (499 kB)\n",
      "   ---------------------------------------- 0.0/499.8 kB ? eta -:--:--\n",
      "   --------------------- ------------------ 266.2/499.8 kB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 430.1/499.8 kB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 499.8/499.8 kB 3.5 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.29.0-cp311-cp311-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 0.6/2.0 MB 12.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.1/2.0 MB 8.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.5/2.0 MB 8.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.0/2.0 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 7.4 MB/s eta 0:00:00\n",
      "Downloading types_requests-2.32.0.20241016-py3-none-any.whl (15 kB)\n",
      "Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "   ---------------------------------------- 0.0/128.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 128.4/128.4 kB 7.9 MB/s eta 0:00:00\n",
      "Installing collected packages: urllib3, pydantic-core, httpx-sse, fastavro, types-requests, cohere\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.16\n",
      "    Uninstalling urllib3-1.26.16:\n",
      "      Successfully uninstalled urllib3-1.26.16\n",
      "Successfully installed cohere-5.13.12 fastavro-1.10.0 httpx-sse-0.4.0 pydantic-core-2.29.0 types-requests-2.32.0.20241016 urllib3-2.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "botocore 1.29.76 requires urllib3<1.27,>=1.25.4, but you have urllib3 2.3.0 which is incompatible.\n",
      "selenium 4.0.0 requires urllib3[secure]~=1.26, but you have urllib3 2.3.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting elasticsearch\n",
      "  Obtaining dependency information for elasticsearch from https://files.pythonhosted.org/packages/80/b6/f0923ed8edddd8e70a8b5271f017944e3f24804b6d115092b97f1e9f7f51/elasticsearch-8.17.1-py3-none-any.whl.metadata\n",
      "  Downloading elasticsearch-8.17.1-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting elastic-transport<9,>=8.15.1 (from elasticsearch)\n",
      "  Obtaining dependency information for elastic-transport<9,>=8.15.1 from https://files.pythonhosted.org/packages/2a/0d/2dd25c06078070973164b661e0d79868e434998391f9aed74d4070aab270/elastic_transport-8.17.0-py3-none-any.whl.metadata\n",
      "  Downloading elastic_transport-8.17.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from elastic-transport<9,>=8.15.1->elasticsearch) (2.3.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\bdion\\anaconda3\\lib\\site-packages (from elastic-transport<9,>=8.15.1->elasticsearch) (2025.1.31)\n",
      "Downloading elasticsearch-8.17.1-py3-none-any.whl (653 kB)\n",
      "   ---------------------------------------- 0.0/654.0 kB ? eta -:--:--\n",
      "   --------------------- ------------------ 348.2/654.0 kB 7.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  645.1/654.0 kB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 654.0/654.0 kB 4.6 MB/s eta 0:00:00\n",
      "Downloading elastic_transport-8.17.0-py3-none-any.whl (64 kB)\n",
      "   ---------------------------------------- 0.0/64.5 kB ? eta -:--:--\n",
      "   -------------------------------------- - 61.4/64.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 64.5/64.5 kB 875.5 kB/s eta 0:00:00\n",
      "Installing collected packages: elastic-transport, elasticsearch\n",
      "Successfully installed elastic-transport-8.17.0 elasticsearch-8.17.1\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu sentence-transformers transformers openai\n",
    "!pip install anthropic\n",
    "!pip install voyageai\n",
    "!pip install cohere\n",
    "!pip install elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# client = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the \"documents\" in openbook.txt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"your_file.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Remove newline characters\n",
    "documents = [line.strip() for line in lines]\n",
    "\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Compute embeddings\n",
    "doc_embeddings = embedder.encode(documents, convert_to_numpy=True)\n",
    "\n",
    "# Build FAISS index\n",
    "index = faiss.IndexFlatL2(doc_embeddings.shape[1])\n",
    "index.add(doc_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_docs(query, k=2):\n",
    "    query_embedding = embedder.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    \n",
    "    retrieved_docs = [documents[i] for i in indices[0]]\n",
    "    return retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query, context):\n",
    "    prompt = f\"Answer the question based on the given context:\\n\\nContext: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "    \n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-opus-20240229\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=300\n",
    "    )\n",
    "    \n",
    "    return response.content[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize a Vector DB Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "client = anthropic.Anthropic(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import voyageai\n",
    "from typing import List, Dict, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "class VectorDB:\n",
    "    def __init__(self, name: str, api_key = None):\n",
    "        if api_key is None:\n",
    "            api_key = os.getenv(\"VOYAGE_API_KEY\")\n",
    "        self.client = voyageai.Client(api_key=api_key)\n",
    "        self.name = name\n",
    "        self.embeddings = []\n",
    "        self.metadata = []\n",
    "        self.query_cache = {}\n",
    "        self.db_path = f\"./data/{name}/vector_db.pkl\"\n",
    "\n",
    "    def load_data(self, dataset: List[Dict[str, Any]]):\n",
    "        if self.embeddings and self.metadata:\n",
    "            print(\"Vector database is already loaded. Skipping data loading.\")\n",
    "            return\n",
    "        if os.path.exists(self.db_path):\n",
    "            print(\"Loading vector database from disk.\")\n",
    "            self.load_db()\n",
    "            return\n",
    "\n",
    "        texts_to_embed = []\n",
    "        metadata = []\n",
    "        total_chunks = sum(len(doc['chunks']) for doc in dataset)\n",
    "        \n",
    "        with tqdm(total=total_chunks, desc=\"Processing chunks\") as pbar:\n",
    "            for doc in dataset:\n",
    "                for chunk in doc['chunks']:\n",
    "                    texts_to_embed.append(chunk['content'])\n",
    "                    metadata.append({\n",
    "                        'doc_id': doc['doc_id'],\n",
    "                        'original_uuid': doc['original_uuid'],\n",
    "                        'chunk_id': chunk['chunk_id'],\n",
    "                        'original_index': chunk['original_index'],\n",
    "                        'content': chunk['content']\n",
    "                    })\n",
    "                    pbar.update(1)\n",
    "\n",
    "        self._embed_and_store(texts_to_embed, metadata)\n",
    "        self.save_db()\n",
    "        \n",
    "        print(f\"Vector database loaded and saved. Total chunks processed: {len(texts_to_embed)}\")\n",
    "\n",
    "    def _embed_and_store(self, texts: List[str], data: List[Dict[str, Any]]):\n",
    "        batch_size = 128\n",
    "        with tqdm(total=len(texts), desc=\"Embedding chunks\") as pbar:\n",
    "            result = []\n",
    "            for i in range(0, len(texts), batch_size):\n",
    "                batch = texts[i : i + batch_size]\n",
    "                batch_result = self.client.embed(batch, model=\"voyage-2\").embeddings\n",
    "                result.extend(batch_result)\n",
    "                pbar.update(len(batch))\n",
    "        \n",
    "        self.embeddings = result\n",
    "        self.metadata = data\n",
    "\n",
    "    def search(self, query: str, k: int = 20) -> List[Dict[str, Any]]:\n",
    "        if query in self.query_cache:\n",
    "            query_embedding = self.query_cache[query]\n",
    "        else:\n",
    "            query_embedding = self.client.embed([query], model=\"voyage-2\").embeddings[0]\n",
    "            self.query_cache[query] = query_embedding\n",
    "\n",
    "        if not self.embeddings:\n",
    "            raise ValueError(\"No data loaded in the vector database.\")\n",
    "\n",
    "        similarities = np.dot(self.embeddings, query_embedding)\n",
    "        top_indices = np.argsort(similarities)[::-1][:k]\n",
    "        \n",
    "        top_results = []\n",
    "        for idx in top_indices:\n",
    "            result = {\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": float(similarities[idx]),\n",
    "            }\n",
    "            top_results.append(result)\n",
    "        \n",
    "        return top_results\n",
    "\n",
    "    def save_db(self):\n",
    "        data = {\n",
    "            \"embeddings\": self.embeddings,\n",
    "            \"metadata\": self.metadata,\n",
    "            \"query_cache\": json.dumps(self.query_cache),\n",
    "        }\n",
    "        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)\n",
    "        with open(self.db_path, \"wb\") as file:\n",
    "            pickle.dump(data, file)\n",
    "\n",
    "    def load_db(self):\n",
    "        if not os.path.exists(self.db_path):\n",
    "            raise ValueError(\"Vector database file not found. Use load_data to create a new database.\")\n",
    "        with open(self.db_path, \"rb\") as file:\n",
    "            data = pickle.load(file)\n",
    "        self.embeddings = data[\"embeddings\"]\n",
    "        self.metadata = data[\"metadata\"]\n",
    "        self.query_cache = json.loads(data[\"query_cache\"])\n",
    "\n",
    "    def validate_embedded_chunks(self):\n",
    "        unique_contents = set()\n",
    "        for meta in self.metadata:\n",
    "            unique_contents.add(meta['content'])\n",
    "    \n",
    "        print(f\"Validation results:\")\n",
    "        print(f\"Total embedded chunks: {len(self.metadata)}\")\n",
    "        print(f\"Unique embedded contents: {len(unique_contents)}\")\n",
    "    \n",
    "        if len(self.metadata) != len(unique_contents):\n",
    "            print(\"Warning: There may be duplicate chunks in the embedded data.\")\n",
    "        else:\n",
    "            print(\"All embedded chunks are unique.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your transformed dataset\n",
    "with open('data/codebase_chunks.json', 'r') as f:\n",
    "    transformed_dataset = json.load(f)\n",
    "\n",
    "# Initialize the VectorDB\n",
    "base_db = VectorDB(\"base_db\")\n",
    "\n",
    "# Load and process the data\n",
    "base_db.load_data(transformed_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic RAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict, Any, Callable, Union\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_jsonl(file_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load JSONL file and return a list of dictionaries.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        return [json.loads(line) for line in file]\n",
    "\n",
    "def evaluate_retrieval(queries: List[Dict[str, Any]], retrieval_function: Callable, db, k: int = 20) -> Dict[str, float]:\n",
    "    total_score = 0\n",
    "    total_queries = len(queries)\n",
    "    \n",
    "    for query_item in tqdm(queries, desc=\"Evaluating retrieval\"):\n",
    "        query = query_item['query']\n",
    "        golden_chunk_uuids = query_item['golden_chunk_uuids']\n",
    "        \n",
    "        # Find all golden chunk contents\n",
    "        golden_contents = []\n",
    "        for doc_uuid, chunk_index in golden_chunk_uuids:\n",
    "            golden_doc = next((doc for doc in query_item['golden_documents'] if doc['uuid'] == doc_uuid), None)\n",
    "            if not golden_doc:\n",
    "                print(f\"Warning: Golden document not found for UUID {doc_uuid}\")\n",
    "                continue\n",
    "            \n",
    "            golden_chunk = next((chunk for chunk in golden_doc['chunks'] if chunk['index'] == chunk_index), None)\n",
    "            if not golden_chunk:\n",
    "                print(f\"Warning: Golden chunk not found for index {chunk_index} in document {doc_uuid}\")\n",
    "                continue\n",
    "            \n",
    "            golden_contents.append(golden_chunk['content'].strip())\n",
    "        \n",
    "        if not golden_contents:\n",
    "            print(f\"Warning: No golden contents found for query: {query}\")\n",
    "            continue\n",
    "        \n",
    "        retrieved_docs = retrieval_function(query, db, k=k)\n",
    "        \n",
    "        # Count how many golden chunks are in the top k retrieved documents\n",
    "        chunks_found = 0\n",
    "        for golden_content in golden_contents:\n",
    "            for doc in retrieved_docs[:k]:\n",
    "                retrieved_content = doc['metadata'].get('original_content', doc['metadata'].get('content', '')).strip()\n",
    "                if retrieved_content == golden_content:\n",
    "                    chunks_found += 1\n",
    "                    break\n",
    "        \n",
    "        query_score = chunks_found / len(golden_contents)\n",
    "        total_score += query_score\n",
    "    \n",
    "    average_score = total_score / total_queries\n",
    "    pass_at_n = average_score * 100\n",
    "    return {\n",
    "        \"pass_at_n\": pass_at_n,\n",
    "        \"average_score\": average_score,\n",
    "        \"total_queries\": total_queries\n",
    "    }\n",
    "\n",
    "def retrieve_base(query: str, db, k: int = 20) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Retrieve relevant documents using either VectorDB or ContextualVectorDB.\n",
    "    \n",
    "    :param query: The query string\n",
    "    :param db: The VectorDB or ContextualVectorDB instance\n",
    "    :param k: Number of top results to retrieve\n",
    "    :return: List of retrieved documents\n",
    "    \"\"\"\n",
    "    return db.search(query, k=k)\n",
    "\n",
    "def evaluate_db(db, original_jsonl_path: str, k):\n",
    "    # Load the original JSONL data for queries and ground truth\n",
    "    original_data = load_jsonl(original_jsonl_path)\n",
    "    \n",
    "    # Evaluate retrieval\n",
    "    results = evaluate_retrieval(original_data, retrieve_base, db, k)\n",
    "    print(f\"Pass@{k}: {results['pass_at_n']:.2f}%\")\n",
    "    print(f\"Total Score: {results['average_score']}\")\n",
    "    print(f\"Total queries: {results['total_queries']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results5 = evaluate_db(base_db, 'data/Main/train.jsonl', 5)\n",
    "results10 = evaluate_db(base_db, 'data/Main/train.jsonl', 10)\n",
    "results20 = evaluate_db(base_db, 'data/Main/train.jsonl', 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
